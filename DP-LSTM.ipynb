{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a06bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import sys\n",
    "import warnings\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dp_accounting\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c81f4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "\n",
    "attr = 'Lane 1 Flow (Veh/5 Minutes)'\n",
    "lags = 12\n",
    "df_train = pd.read_csv(trian_path, encoding='utf-8').fillna(0)\n",
    "df_test = pd.read_csv(test_path, encoding='utf-8').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36be9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1)).fit(df_train[attr].values.reshape(-1, 1))\n",
    "flow1 = scaler.transform(df_train[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "flow2 = scaler.transform(df_test[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "train, test = [], []\n",
    "for i in range(lags, len(flow1)):\n",
    "    train.append(flow1[i - lags: i + 1])\n",
    "for i in range(lags, len(flow2)):\n",
    "    test.append(flow2[i - lags: i + 1])\n",
    "\n",
    "train = np.array(train).astype(np.float32)\n",
    "train = np.expand_dims(train, axis=-1)\n",
    "test = np.array(test).astype(np.float32)\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "np.random.shuffle(train)\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0a3e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7764, 12, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c7486b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7764, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c7376c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_NUM = 50\n",
    "LOCAL_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "GLOBAL_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192c47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分发数据\n",
    "train_data, test_data, val_data = [], [], []\n",
    "for edge_ids in range(EDGE_NUM):\n",
    "    data_length = X_train.shape[0] // EDGE_NUM #边缘节点数据长度\n",
    "    temp_data = X_train[data_length*edge_ids:data_length*edge_ids+data_length]\n",
    "    tf.expand_dims(temp_data,axis=-1)\n",
    "    temp_label = y_train[data_length*edge_ids:data_length*edge_ids+data_length]\n",
    "    temp_dataset = tf.data.Dataset.from_tensor_slices((temp_data, temp_label)).repeat(LOCAL_EPOCHS).batch(BATCH_SIZE)\n",
    "    train_data.append(temp_dataset)\n",
    "temp_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_data.append(temp_dataset.batch(BATCH_SIZE))\n",
    "temp_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_data.append(temp_dataset.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b48cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 12, 1), (None, 1)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cef2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_spec():\n",
    "    return (\n",
    "        tf.TensorSpec([None, 12, 1], tf.float32),\n",
    "        tf.TensorSpec([None, 1], tf.float32)\n",
    "    )\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(64, input_shape=(12, 1), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    return tff.learning.from_keras_model(\n",
    "        model,\n",
    "        input_spec=input_spec(),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "953dff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2cc029",
   "metadata": {},
   "source": [
    "#### DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1535fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_clients = EDGE_NUM\n",
    "clients_per_thread = 5\n",
    "# tff.backends.native.set_sync_local_cpp_execution_context(\n",
    "#     max_concurrent_computation_calls=total_clients / clients_per_thread)\n",
    "\n",
    "def train(rounds, noise_multiplier, clients_per_round, data_frame):\n",
    "  # Using the `dp_aggregator` here turns on differential privacy with adaptive\n",
    "  # clipping.\n",
    "  aggregation_factory = tff.learning.model_update_aggregator.dp_aggregator(\n",
    "      noise_multiplier, clients_per_round)\n",
    "\n",
    "  # We use Poisson subsampling which gives slightly tighter privacy guarantees\n",
    "  # compared to having a fixed number of clients per round. The actual number of\n",
    "  # clients per round is stochastic with mean clients_per_round.\n",
    "  sampling_prob = clients_per_round / total_clients\n",
    "\n",
    "  # Build a federated averaging process.\n",
    "  # Typically a non-adaptive server optimizer is used because the noise in the\n",
    "  # updates can cause the second moment accumulators to become very large\n",
    "  # prematurely.\n",
    "  learning_process = tff.learning.algorithms.build_unweighted_fed_avg(\n",
    "        model_fn,\n",
    "        client_optimizer_fn=lambda: tf.keras.optimizers.Adam(),\n",
    "        server_optimizer_fn=lambda: tf.keras.optimizers.SGD(1.0, momentum=0.9),\n",
    "        model_aggregator=aggregation_factory)\n",
    "\n",
    "  eval_process = tff.learning.build_federated_evaluation(model_fn)\n",
    "\n",
    "  # Training loop.\n",
    "  state = learning_process.initialize()\n",
    "  for round in range(rounds):\n",
    "    if round % 5 == 0:\n",
    "      model_weights = learning_process.get_model_weights(state)\n",
    "      metrics = eval_process(model_weights, [test_data])['eval']\n",
    "      if round < 25 or round % 25 == 0:\n",
    "        print(f'Round {round:3d}: {metrics}')\n",
    "      data_frame = data_frame.append({'Round': round,\n",
    "                                      'NoiseMultiplier': noise_multiplier,\n",
    "                                      **metrics}, ignore_index=True)\n",
    "\n",
    "    # Sample clients for a round. Note that if your dataset is large and\n",
    "    # sampling_prob is small, it would be faster to use gap sampling.\n",
    "    x = np.random.uniform(size=total_clients)\n",
    "#     sampled_clients = [\n",
    "#         train_data.client_ids[i] for i in range(total_clients)\n",
    "#         if x[i] < sampling_prob]\n",
    "    sampled_train_data = [\n",
    "        train_data[i] for i in range(total_clients) if x[i] < sampling_prob]\n",
    "\n",
    "    # Use selected clients for update.\n",
    "    result = learning_process.next(state, sampled_train_data)\n",
    "    state = result.state\n",
    "    metrics = result.metrics\n",
    "\n",
    "  model_weights = learning_process.get_model_weights(state)\n",
    "  metrics = eval_process(model_weights, [test_data])['eval']\n",
    "  print(f'Round {rounds:3d}: {metrics}')\n",
    "  data_frame = data_frame.append({'Round': rounds,\n",
    "                                  'NoiseMultiplier': noise_multiplier,\n",
    "                                  **metrics}, ignore_index=True)\n",
    "\n",
    "  return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ab75c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with noise multiplier: 0.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_federated.python.learning' has no attribute 'model_update_aggregator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m noise_multiplier \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.75\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]:\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStarting training with noise multiplier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnoise_multiplier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m   data_frame \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients_per_round\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn [12], line 9\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(rounds, noise_multiplier, clients_per_round, data_frame)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(rounds, noise_multiplier, clients_per_round, data_frame):\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;66;03m# Using the `dp_aggregator` here turns on differential privacy with adaptive\u001b[39;00m\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;66;03m# clipping.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m   aggregation_factory \u001b[38;5;241m=\u001b[39m \u001b[43mtff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_update_aggregator\u001b[49m\u001b[38;5;241m.\u001b[39mdp_aggregator(\n\u001b[0;32m     10\u001b[0m       noise_multiplier, clients_per_round)\n\u001b[0;32m     12\u001b[0m   \u001b[38;5;66;03m# We use Poisson subsampling which gives slightly tighter privacy guarantees\u001b[39;00m\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;66;03m# compared to having a fixed number of clients per round. The actual number of\u001b[39;00m\n\u001b[0;32m     14\u001b[0m   \u001b[38;5;66;03m# clients per round is stochastic with mean clients_per_round.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m   sampling_prob \u001b[38;5;241m=\u001b[39m clients_per_round \u001b[38;5;241m/\u001b[39m total_clients\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_federated.python.learning' has no attribute 'model_update_aggregator'"
     ]
    }
   ],
   "source": [
    "data_frame = pd.DataFrame()\n",
    "rounds = 100\n",
    "clients_per_round = 50\n",
    "\n",
    "for noise_multiplier in [0.0, 0.5, 0.75, 1.0]:\n",
    "  print(f'Starting training with noise multiplier: {noise_multiplier}')\n",
    "  data_frame = train(rounds, noise_multiplier, clients_per_round, data_frame)\n",
    "  print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed39",
   "language": "python",
   "name": "fed39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
