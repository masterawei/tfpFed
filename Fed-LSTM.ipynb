{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a06bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import sys\n",
    "import warnings\n",
    "import argparse\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dp_accounting\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81f4ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trian_path = \"train.csv\"\n",
    "test_path = \"test.csv\"\n",
    "\n",
    "attr = 'Lane 1 Flow (Veh/5 Minutes)'\n",
    "lags = 12\n",
    "df_train = pd.read_csv(trian_path, encoding='utf-8').fillna(0)\n",
    "df_test = pd.read_csv(test_path, encoding='utf-8').fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be9048",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1)).fit(df_train[attr].values.reshape(-1, 1))\n",
    "flow1 = scaler.transform(df_train[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "flow2 = scaler.transform(df_test[attr].values.reshape(-1, 1)).reshape(1, -1)[0]\n",
    "\n",
    "train, test = [], []\n",
    "for i in range(lags, len(flow1)):\n",
    "    train.append(flow1[i - lags: i + 1])\n",
    "for i in range(lags, len(flow2)):\n",
    "    test.append(flow2[i - lags: i + 1])\n",
    "\n",
    "train = np.array(train).astype(np.float32)\n",
    "train = np.expand_dims(train, axis=-1)\n",
    "test = np.array(test).astype(np.float32)\n",
    "test = np.expand_dims(test, axis=-1)\n",
    "np.random.shuffle(train)\n",
    "\n",
    "X_train = train[:, :-1]\n",
    "y_train = train[:, -1]\n",
    "X_test = test[:, :-1]\n",
    "y_test = test[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a3e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7486b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7376c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EDGE_NUM = 50\n",
    "LOCAL_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "GLOBAL_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192c47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#分发数据\n",
    "train_data, test_data, val_data = [], [], []\n",
    "for edge_ids in range(EDGE_NUM):\n",
    "    data_length = X_train.shape[0] // EDGE_NUM #边缘节点数据长度\n",
    "    temp_data = X_train[data_length*edge_ids:data_length*edge_ids+data_length]\n",
    "    tf.expand_dims(temp_data,axis=-1)\n",
    "    temp_label = y_train[data_length*edge_ids:data_length*edge_ids+data_length]\n",
    "    temp_dataset = tf.data.Dataset.from_tensor_slices((temp_data, temp_label)).repeat(LOCAL_EPOCHS).batch(BATCH_SIZE)\n",
    "    train_data.append(temp_dataset)\n",
    "temp_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "val_data.append(temp_dataset.batch(BATCH_SIZE))\n",
    "temp_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "test_data.append(temp_dataset.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b48cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cef2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_spec():\n",
    "    return (\n",
    "        tf.TensorSpec([None, 12, 1], tf.float32),\n",
    "        tf.TensorSpec([None, 1], tf.float32)\n",
    "    )\n",
    "\n",
    "def model_fn():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(64, input_shape=(12, 1), return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "\n",
    "    return tff.learning.from_keras_model(\n",
    "        model,\n",
    "        input_spec=input_spec(),\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953dff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = tff.learning.build_federated_evaluation(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.Adam(),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1),\n",
    "    # model_update_aggregation_factory=zeroing_mean,\n",
    "    # broadcast_process=encoded_broadcast_process,\n",
    "    # aggregation_process=encoded_mean_process\n",
    ")\n",
    "\n",
    "state = trainer.initialize()\n",
    "train_hist = []\n",
    "losses = []\n",
    "accs = []\n",
    "\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "# environment = set_sizing_environment()\n",
    "for i in range(GLOBAL_EPOCHS):\n",
    "    state, metrics = trainer.next(state, train_data)\n",
    "    train_hist.append(metrics)\n",
    "    losses.append(metrics['train']['loss'])\n",
    "    accs.append(metrics['train']['rmse'])\n",
    "    ''' 通信传输比特 '''\n",
    "#     size_info = environment.get_size_info()\n",
    "#     broadcasted_bits = size_info.broadcast_bits[-1]\n",
    "#     aggregated_bits = size_info.aggregate_bits[-1]\n",
    "    val_metrics = evaluator(state.model, val_data)\n",
    "    val_losses.append(val_metrics['loss'])\n",
    "    val_accs.append(val_metrics['rmse'])\n",
    "    # print(f\"\\rRun {i+1}/{GLOBAL_EPOCHS} _ loss={metrics['train']['loss']} _ \"\n",
    "    #       f\"RMSE={metrics['train']['rmse']}_\"\n",
    "    #       f\"broadcasted_bits={format_size(broadcasted_bits)}_\"\n",
    "    #       f\"aggregated_bits={format_size(aggregated_bits)}\")\n",
    "    print(f\"\\rRun {i+1}/{GLOBAL_EPOCHS} _ loss={metrics['train']['loss']} _ \"\n",
    "          f\"RMSE={metrics['train']['rmse']}_\"f\"valLoss={val_metrics['loss']}_\"f\"valRmse\"\n",
    "          f\"={val_metrics['rmse']}_\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fed39",
   "language": "python",
   "name": "fed39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
